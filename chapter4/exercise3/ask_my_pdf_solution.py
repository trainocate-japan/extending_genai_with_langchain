import os
from dotenv import load_dotenv, find_dotenv
import tempfile
import streamlit as st
from streamlit_feedback import streamlit_feedback
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_community.callbacks.manager import get_openai_callback
from langchain_core.tracers.context import collect_runs
from langchain_pinecone import PineconeVectorStore
import pinecone
from pinecone import ServerlessSpec
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_community.llms import OpenAI
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever
from streamlit.runtime.scriptrunner_utils.script_run_context import get_script_run_ctx
from langsmith import Client

# API ã‚­ãƒ¼ãªã©ã®è¨­å®š
# python-dotenv ã‚’ä½¿ç”¨ã—ã¦ã€.env ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚ŒãŸ API ã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®šã™ã‚‹
load_dotenv(find_dotenv(), override=True)
os.environ.get('OPENAI_API_KEY')
os.environ.get('PINECONE_API_KEY')
os.environ.get('LANGCHAIN_API_KEY')
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "default"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

# Streamlit ã§ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã®åˆæœŸåŒ–
if 'costs' not in st.session_state:
    st.session_state.costs = []
if 'latest_run_id' not in st.session_state:
    st.session_state.latest_run_id = None
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'feedback_submitted' not in st.session_state:
    st.session_state.feedback_submitted = False

# Stremlit ã§ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ ID ã®å–å¾—
ctx = get_script_run_ctx()

# CHECK
# Chat history ã®åˆæœŸåŒ–
# ã“ã“ã§ã¯ Streamlit ã®æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ StreamlitChatMessageHistory ã‚’ä½¿ç”¨ã—ã¾ã™
chat_history = StreamlitChatMessageHistory(key="langchain_messages")

def init_page():
    st.set_page_config(
        page_title="Ask My PDF",
        page_icon="ğŸ“–"
    )
    st.sidebar.title("Nav")

# PDF ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# st.file_uploader ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
# load_document é–¢æ•°ã¨ chunk_data é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
def get_pdf_text():
    uploaded_file = st.file_uploader(
        label='Upload your PDF hereğŸ˜‡',
        type='pdf',
        accept_multiple_files=True
    )
    if uploaded_file:
        if isinstance(uploaded_file, list):
            data_list = []
            for file in uploaded_file:
                data = load_document(file)
                if data:
                    data_list.extend(data)
            if data_list:
                chunks = chunk_data(data_list)
                return chunks
        else:
            data = load_document(uploaded_file)
            if data:
                chunks = chunk_data(data)
                return chunks
    return None

# get_pdf_text ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¦ Document loader (PyPDFLoader) ã‚’å®Ÿè¡Œ
def load_document(file):
    name, extension = os.path.splitext(file.name)

    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(file.getvalue())
        tmp_file_path = tmp_file.name

    if extension == '.pdf':
        # TASK
        # ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ PyPDFLoader ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„
        # file_path=tmp_file_path
        loader = PyPDFLoader(file_path=tmp_file_path)
        data = loader.load()
        return data
    else:
        st.write('Document format is not supported!')
        return None

# get_pdf_text ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¦ Text splitter (RecursiveCharacterTextSplitter) ã‚’å®Ÿè¡Œ
def chunk_data(data, chunk_size=1024, chunk_overlap=256):
    # TASK
    # RecursiveCharacterTextSplitter ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = text_splitter.split_documents(data)
    return chunks

# Vector store ã®ä½œæˆ
# å…·ä½“çš„ãªå‡¦ç†ã¯ insert_or_fetch_embeddings é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å®Ÿè¡Œ
def build_vector_store(chunks):
    index_name = "askadocument"
    vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)
    return vector_store

# build_vector_store ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¦ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ–°è¦ä½œæˆã¨ Vector store ã®ä½œæˆã‚’è¡Œã†
def insert_or_fetch_embeddings(index_name, chunks):
    pc = pinecone.Pinecone()
    # TASK
    # ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ Embedding model (OpenAIEmbeddings) ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„
    # model='text-embedding-3-small'
    # dimensions=1536
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)

    # Create or update the index if it doesn't exist
    if index_name not in pc.list_indexes().names():
        with st.spinner("Creating index and embeddings ... "):
            pc.create_index(
                name=index_name,
                dimension=1536,
                metric='cosine',
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )
            # CHECK
            # Vector store (Pinecone) ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)
    else:
        with st.spinner("Updating embeddings ... "):
            vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)
    return vector_store

# æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
def delete_pinecone_index(index_name='all'):
    pc = pinecone.Pinecone()
    if index_name == 'all':
        indexes = pc.list_indexes().names()
        for index in indexes:
            pc.delete_index(index)

# PDF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç”»é¢ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
# ä¸Šè¨˜ã®å„é–¢æ•°ãŒå‘¼ã³å‡ºã•ã‚Œã¦ã„ã‚‹
def page_pdf_upload_and_build_vector_db():
    st.title("PDF Upload")
    container = st.container()
    with container:
        st.markdown("## Upload PDF files")
        chunks = get_pdf_text()
        if chunks:
            with st.spinner("Loading PDF ..."):
                vector_store = build_vector_store(chunks)
                st.session_state.vector_store = vector_store
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤ã™ã‚‹ãƒœã‚¿ãƒ³
        # delete_pinecone_index é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
        st.markdown("## Delete Index")
        if st.button("Delete Index"):
            delete_pinecone_index()
            st.session_state.vector_store = None
            st.success("Vector store index deleted successfully!")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”»é¢ä¸Šã§é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã™ã‚‹
def select_model():
    model = st.sidebar.radio("Choose a model:", ("GPT-4o-mini", "GPT-4o"))
    if model == "GPT-4o-mini":
        st.session_state.model_name = "gpt-4o-mini"
    else:
        st.session_state.model_name = "gpt-4o"
    # TASK
    # ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ Chat model (ChatOpenAI) ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„
    # temperature=0
    # model=st.session_state.model_name
    model = ChatOpenAI(temperature=0, model=st.session_state.model_name)
    return model

# page_ask_my_pdf ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã€RAG Chain ã‚’æ§‹æˆã—ã¦å®Ÿè¡Œã™ã‚‹
def get_answer_with_history(model, vector_store, query, session_id='unused'):
    # TASK
    # ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ Retriever ã‚’ä½œæˆã—ã¦ãã ã•ã„
    # search_type='similarity'
    # search_kwargs={'k': 10}
    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 10})

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )

    # TASK
    # ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢ãƒ»åå¾—ã‚’è¡Œã† Retriever ã‚’ä½œæˆã—ã¦ãã ã•ã„
    # çµ„ã¿è¾¼ã¿ã® create_histroy_aware_retriever é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™
    history_aware_retriever = create_history_aware_retriever(
        model, retriever, contextualize_q_prompt
    )

    # Chat prompt ã®ä½œæˆ
    system_message = """ä»¥ä¸‹ã®å‚è€ƒç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«ä¸å¯§ã‹ã¤è¦ªåˆ‡ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚‚ã—å‚è€ƒç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸­ã«å›ç­”ã«å½¹ç«‹ã¤æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°ã€åˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

    {context}"""
    human_message = "è³ªå•ï¼š{input}"

    # TASK
    # ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã® Prompt template ã‚’ä½œæˆã—ã¦ãã ã•ã„
    # MessagePlaceholder ã‚’ä½¿ç”¨ã—ã¦ chat_history ãŒæŒ¿å…¥ã•ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„
    chat_prompt = ChatPromptTemplate.from_messages([

        (
            "system", system_message
        ),
        MessagesPlaceholder("chat_history"),
        (
            "human", human_message
        )
    ])

    # Retriever ã«ã‚ˆã‚‹æ¤œç´¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
    add_context = RunnablePassthrough.assign(context=history_aware_retriever)

    # TASK
    # RAG ã®å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ Chain ã‚’å®šç¾©ã—ã¦ãã ã•ã„
    # add_context > chat_prompt > model > StrOutputParser ã®é †ã«å‡¦ç†ã•ã‚Œã‚‹ã‚ˆã†ã«å®šç¾©ã—ã¾ã™
    rag_chain = add_context | chat_prompt | model | StrOutputParser()

    runnable_with_history = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: chat_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )
    with get_openai_callback() as cb:
        with collect_runs() as runs_cb:
            answer = runnable_with_history.invoke({'input': query}, config={"configurable": {"session_id": session_id}})
            run_id = runs_cb.traced_runs[0].id
            st.session_state.latest_run_id = run_id
    return answer, cb.total_cost

# ãƒãƒ£ãƒƒãƒˆç”»é¢ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def page_ask_my_pdf():
    st.title("ğŸ“– Ask My PDF(s)")

    """
    The messages are stored in Session State across re-runs automatically.
    You can view the contents of Session State in the expander below.
    """
    view_messages = st.expander("View the message contents in session state")

    # select_model é–¢æ•°å†…ã§ Chat model ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦å–å¾—
    model = select_model()

    if "vector_store" in st.session_state:
        vector_store = st.session_state.vector_store
    else:
        vector_store = None

    if vector_store:
        for msg in chat_history.messages:
            st.chat_message(msg.type).write(msg.content)

        if query := st.chat_input():
            st.chat_message("human").write(query)
            with st.spinner("ChatGPT is typing ..."):
                # è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’å–å¾—
                answer, cost = get_answer_with_history(model, vector_store, query, session_id=ctx.session_id)
            # å›ç­”ã‚’è¡¨ç¤º
            st.chat_message("ai").write(answer)
            # ã‚³ã‚¹ãƒˆã‚’åŠ ç®—
            st.session_state.costs.append(cost)
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.feedback_submitted = False
            # æœ€æ–°ã® run_id ã‚’ä¿æŒ
            # st.session_state.latest_run_id = st.session_state.latest_run_id

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ•ã‚©ãƒ¼ãƒ ã®è¡¨ç¤º
        # streamlit_feedback ã§ãƒ•ã‚©ãƒ¼ãƒ ã‚’è¡¨ç¤ºã—ã€send_feedback é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡
        if st.session_state.get("latest_run_id") and not st.session_state.feedback_submitted:
            run_id = st.session_state.latest_run_id
            feedback = streamlit_feedback(
                feedback_type="thumbs",
                optional_text_label="[Optional] Please provide an explanation",
                on_submit=send_feedback,
                key=f"fb_k_{run_id}",
                args=[run_id]
            )

    # Draw the messages at the end, so newly generated ones show up immediately
    with view_messages:
        """
        Message History initialized with:
        chat_history = StreamlitChatMessageHistory(key="langchain_messages")
        
        Contents of `st.session_state.langchain_messages`:
        """
        view_messages.json(st.session_state.langchain_messages)

# LangSmith ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡ã™ã‚‹é–¢æ•°
def send_feedback(user_feedback, run_id):
    scores = {"ğŸ‘": 1, "ğŸ‘": 0}
    score_key = user_feedback["score"]
    score = scores[score_key]
    comment = user_feedback.get("text")

    # LangSmith API ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡
    client = Client()
    client.create_feedback(
        run_id=run_id,
        key="thumbs",
        score=score,
        comment=comment,
    )

    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é€ä¿¡ãŒå®Œäº†ã—ãŸã“ã¨ã‚’è¨˜éŒ²
    st.session_state.feedback_submitted = True
    st.success("Thank you for your feedback!")

# main é–¢æ•°
# ã¾ãšæœ€åˆã«å®Ÿè¡Œã•ã‚Œã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ç”»é¢å…¨ä½“ã®éª¨æ ¼ã‚’æ§‹æˆã™ã‚‹
def main():
    init_page()

    selection = st.sidebar.radio("Go to", ["PDF Upload", "Ask My PDF(s)"])
    if selection == "PDF Upload":
        page_pdf_upload_and_build_vector_db()
    elif selection == "Ask My PDF(s)":
        page_ask_my_pdf()

    costs = st.session_state.get('costs', [])
    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${sum(costs):.5f}**")
    for cost in costs:
        st.sidebar.markdown(f"- ${cost:.5f}")

if __name__ == '__main__':
    main()
