{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcRuFK5W4/x/S5J6L3ayPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trainocate-japan/extending_genai_with_langchain/blob/main/chapter4/implement_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: ハンズオンの準備\n",
        "---"
      ],
      "metadata": {
        "id": "2SXS3qjHO5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "6NvuYwKzaYt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain langchain-openai langchain-community langchain-core langchain-text-splitters"
      ],
      "metadata": {
        "id": "xNJhQoCptr7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.3.7 langchain-openai==0.2.9 langchain-community==0.3.7 langchain-core==0.3.18 langchain-text-splitters==0.3.2"
      ],
      "metadata": {
        "id": "5L-ErUJuMtEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep langchain"
      ],
      "metadata": {
        "id": "15QyqnKk1CTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API キーの設定\n",
        "\n",
        "*  左ナビゲーションで [**シークレット**] アイコン (鍵形のアイコン) をクリックします。\n",
        "*  [**新しいシークレットを追加**] をクリックし、`LANGCHAIN_API_KEY`、`OPENAI_API_KEY`、`PINECONE_API_KEY` の 3 つを設定し、[**ノートブックからのアクセス**] を有効にします。\n",
        "  *  `OPENAI_API_KEY` の [**値**] には指定されたキーを入力します。\n",
        "  *  `LANGCHAIN_API_KEY` と `PINECONE_API_KEY` の [**値**] にはご自身で取得したキーを入力してください。\n",
        "*  入力が完了したら、下のセルを実行します。"
      ],
      "metadata": {
        "id": "7Xq-T4gzi4ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3-Ha_aLspoO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## サンプルファイルのアップロード\n",
        "\n",
        "*  左ナビゲーションで [**ファイル**] アイコンをクリックします。\n",
        "*  開いた [ファイル] 欄の何もない部分で右クリックし、[**新しいフォルダ**] をクリックします。(sample_data と同じ階層にフォルダが作られるようにします)\n",
        "*  作成されたフォルダに **files** という名前を設定します。\n",
        "*  files フォルダにカーソルを合わせ、3 点リーダアイコンをクリックして、[**アップロード**] をクリックします。\n",
        "*  ローカルの files フォルダにあるすべてのファイルを選択してアップロードします。\n",
        "*  「警告」のポップアップが表示されますが問題ありません。[**OK**] をクリックしてポップアップを閉じます。"
      ],
      "metadata": {
        "id": "FH5mLQyujAyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pclZg_IOh173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: LangChain による RAG の実装\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5TZLcXa5sxwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document loaders\n",
        "---\n",
        "https://python.langchain.com/docs/concepts/#document-loaders\n",
        "https://python.langchain.com/docs/integrations/document_loaders/"
      ],
      "metadata": {
        "id": "IORhWlqn8Fgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextLoader - テキストファイルのロード\n",
        "[langchain_community.document_loaders.text.TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html)"
      ],
      "metadata": {
        "id": "O8mtRsNxExM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3sdyngepk-"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.text import TextLoader\n",
        "\n",
        "file = 'files/gingatetsudono_yoru.txt'\n",
        "print(f'Loading {file}')\n",
        "loader = TextLoader(file)\n",
        "data = loader.load() # Document オブジェクトのリストが返される\n",
        "\n",
        "# data\n",
        "# print(data[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyPDFLoader - PDF ファイルのロード\n",
        "[langchain_community.document_loaders.pdf.PyPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "hNXYD6YlAN_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf"
      ],
      "metadata": {
        "id": "_N9c2mNslpaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "file = 'files/employment_regulations.pdf'\n",
        "print(f'Loading {file}')\n",
        "loader = PyPDFLoader(file)\n",
        "data = loader.load()\n",
        "\n",
        "# data\n",
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "zJzkCthDjxqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Docx2txtLoader - DOCX ファイルのロード\n",
        "[langchain_community.document_loaders.word_document.Docx2txtLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.word_document.Docx2txtLoader.html)"
      ],
      "metadata": {
        "id": "TpSBwtpuoGnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q docx2txt"
      ],
      "metadata": {
        "id": "WedKS69VpIPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
        "\n",
        "file = 'files/the_great_gatsby.docx'\n",
        "print(f'Loading {file}')\n",
        "loader = Docx2txtLoader(file)\n",
        "data = loader.load()\n",
        "\n",
        "# print(data[0].page_content)"
      ],
      "metadata": {
        "id": "VFhLPHBIoPpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WikipediaLoader\n",
        "[langchain_community.document_loaders.wikipedia.WikipediaLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.wikipedia.WikipediaLoader.html)"
      ],
      "metadata": {
        "id": "K98_7Bn5putC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wikipedia"
      ],
      "metadata": {
        "id": "dQxN_J7orXHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
        "\n",
        "query = 'ディープラーニング'\n",
        "lang = 'ja'\n",
        "load_max_docs = 2\n",
        "\n",
        "loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "data = loader.load()\n",
        "\n",
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "6EvaJ2R7qJPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これら以外にも、様々なソースからデータを取得するための Document loader を利用できる。  \n",
        "https://python.langchain.com/docs/integrations/document_loaders/"
      ],
      "metadata": {
        "id": "y769dtIpDV7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "あらためて、サンプルの社内規程を PDF からロードする。"
      ],
      "metadata": {
        "id": "bBBDuMQNRsPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "file = 'files/employment_regulations.pdf'\n",
        "print(f'Loading {file}')\n",
        "loader = PyPDFLoader(file)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "waYHDQ2BRjXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text splitters\n",
        "---\n",
        "https://python.langchain.com/docs/concepts/#text-splitters  \n",
        "https://python.langchain.com/docs/how_to/#text-splitters\n",
        "  \n",
        "---\n",
        "### チャンク分割のテスト用サイト\n",
        "- [ChunkViz](https://chunkviz.up.railway.app/)  \n",
        "- [Text Splitter Playground](https://langchain-text-splitter.streamlit.app/)"
      ],
      "metadata": {
        "id": "zonwWRVNFPmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "[langchain_text_splitters.character.RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
      ],
      "metadata": {
        "id": "0IR1R8m9FAbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(data) # 各チャンクの Document のリストを返す\n",
        "\n",
        "# chunks\n",
        "# print(len(chunks))\n",
        "print(chunks[1].page_content)"
      ],
      "metadata": {
        "id": "zVzLWK95zWa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding models\n",
        "---  \n",
        "https://python.langchain.com/docs/concepts/#embedding-models  \n",
        "https://python.langchain.com/docs/integrations/text_embedding/\n"
      ],
      "metadata": {
        "id": "6M977dOaNgtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tiktoken を使って Embedding のコストを見積もる\n",
        "tiktoken は Text splitter で分割したチャンクのリストを入力として受け取り、指定のモデルで embedding を実行した場合に処理されるトークン数を算出して出力する。"
      ],
      "metadata": {
        "id": "IEMlSkZO27Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "total_tokens = sum([len(enc.encode(page.page_content)) for page in chunks])\n",
        "# check prices here: https://openai.com/pricing\n",
        "print(f'Total Tokens: {total_tokens}')\n",
        "print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
      ],
      "metadata": {
        "id": "XiyH3_KAPb4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAIEmbeddings\n",
        "[langchain_openai.embeddings.base.OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)"
      ],
      "metadata": {
        "id": "-Py5CxxfTTvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)"
      ],
      "metadata": {
        "id": "ZhK_HfK5TcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Embidding model が作成される。\n",
        "*   ここではまだ embedding の処理が実行されたわけではなく、次のステップで Vector store に Index を作成する際にこの Embedding model を使用する。"
      ],
      "metadata": {
        "id": "DTHDzKSO9Zes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 【参考】embedding によってベクトル化されたデータはどのような形をしているか"
      ],
      "metadata": {
        "id": "FzpKSVDW-wSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ディープラーニング（英: deep learning）または深層学習（しんそうがくしゅう）とは、対象の全体像から細部までの各々の粒度の概念を階層構造として関連させて学習する手法のことである。\"\n",
        "\n",
        "# embed_query メソッドはテキストデータを入力として受け取り、変換されたベクトルを出力する\n",
        "vector = embeddings.embed_query(text)\n",
        "\n",
        "print(len(vector))\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "EtVcTh2gSCOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI 以外にも、クラウド等で提供される様々な Embeddings model を利用することができる。  \n",
        "https://python.langchain.com/docs/integrations/text_embedding/"
      ],
      "metadata": {
        "id": "P9OPFOZdD0z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector stores (Pinecone を使用する場合)\n",
        "---\n",
        "https://python.langchain.com/docs/concepts/#vector-stores  \n",
        "https://python.langchain.com/docs/integrations/vectorstores/  \n",
        "https://python.langchain.com/docs/integrations/vectorstores/pinecone/  \n",
        "[langchain_pinecone.vectorstores.PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)"
      ],
      "metadata": {
        "id": "kTMlmRP50e-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q pinecone langchain-pinecone"
      ],
      "metadata": {
        "id": "EBuqELtpHkco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze | grep pinecone"
      ],
      "metadata": {
        "id": "1PBEsFUzHVgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pinecone==5.4.0 langchain-pinecone==0.1.2"
      ],
      "metadata": {
        "id": "QgCBZs_ZDztX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "もし pip の依存解決エラーが表示されても無視して構いません。"
      ],
      "metadata": {
        "id": "E-kvkSNIsHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from pinecone import PodSpec, ServerlessSpec"
      ],
      "metadata": {
        "id": "73Zs1U9TF4Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 既存の Index を削除する\n",
        "Pinecone の無料プランでは Index を 1 つしか作成できないため、既存の Index がある場合はいったん削除する。"
      ],
      "metadata": {
        "id": "OJ86Nzz5FSof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')"
      ],
      "metadata": {
        "id": "dHweKFFtF_cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete_pinecone_index()"
      ],
      "metadata": {
        "id": "C0ymBO31ExBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index と Vector store の作成"
      ],
      "metadata": {
        "id": "bvWGmFPiw4m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc = pinecone.Pinecone()\n",
        "\n",
        "index_name = 'regulation'\n",
        "\n",
        " # creating the index and embedding the chunks into the index\n",
        "print(f'Creating index {index_name} and embeddings ... ')\n",
        "\n",
        "# creating a new index\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=1536,\n",
        "    metric='cosine',\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")\n",
        "\n",
        "# processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "# inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)\n",
        "\n",
        "print('Ok')"
      ],
      "metadata": {
        "id": "FQfL3k99AWFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinecone 以外にも様々な Vector store を利用することができる。  \n",
        "https://python.langchain.com/docs/integrations/vectorstores/  \n",
        "\n",
        "これ以外にも、各クラウドで Vector store として利用できるデータベースが提供されており、LangChain で使用するためのライブラリも提供されている。\n"
      ],
      "metadata": {
        "id": "ZWO525SHCNBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrievers\n",
        "---\n",
        "https://python.langchain.com/docs/concepts/#retrievers"
      ],
      "metadata": {
        "id": "CiQf5czUM9a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever の作成"
      ],
      "metadata": {
        "id": "WRb0AO_sCe7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 類似度の高いチャンクを何個取得するかを指定\n",
        "k = 5\n",
        "\n",
        "# Vector store から Retriever を作成\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})"
      ],
      "metadata": {
        "id": "C7hfYZY9FXgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain\n",
        "---"
      ],
      "metadata": {
        "id": "JFnhif5ZGUO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat model と Prompt template の作成"
      ],
      "metadata": {
        "id": "n6a3MyRSDd2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "m2gTAG7EG5qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ],
      "metadata": {
        "id": "8BjLkfymPN0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_template(\"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "{context}\n",
        "\n",
        "質問：{query}\"\"\")"
      ],
      "metadata": {
        "id": "H-dPyHK-Gkd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "{context}\"\"\"\n",
        "human_message = \"質問：{query}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\", system_message\n",
        "    ),\n",
        "    (\n",
        "        \"human\", human_message\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "afHk0odrMX_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain の作成と実行  \n",
        "上で作成したコンポーネントを組み合わせて Chain を定義し、実行する"
      ],
      "metadata": {
        "id": "nMJoNyeXD2Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain を定義\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"query\":RunnablePassthrough(),\n",
        "        \"context\":retriever\n",
        "    }\n",
        "    |chat_prompt\n",
        "    |model\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "# クエリ\n",
        "query = \"国内出張の宿泊費の上限はいくらですか。\"\n",
        "# query = \"育児休暇を取ることはできますか。\"\n",
        "# query = \"海外出張の出張手当は1日当たりいくらですか。\"\n",
        "# query = \"介護休職中に会社から何らかのサポートを受けることはできますか。\"\n",
        "\n",
        "\n",
        "# Chain の実行\n",
        "result = qa_chain.invoke(query)\n",
        "# result = qa_chain.invoke(query, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QMJwlNaAHYEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chain への入力を辞書にする場合"
      ],
      "metadata": {
        "id": "fVHKjnLZEz6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"query\": itemgetter(\"query\")|RunnablePassthrough(),\n",
        "        \"context\": itemgetter(\"query\")|retriever\n",
        "    }\n",
        "    |chat_prompt\n",
        "    |model\n",
        "    |StrOutputParser()\n",
        ")\n",
        "\n",
        "query = \"国内出張の宿泊費の上限はいくらですか。\"\n",
        "\n",
        "# result = qa_chain.invoke(query, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "LLI517Z2QgSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnableParallel"
      ],
      "metadata": {
        "id": "1un581wkFO_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下の部分で何をやっているか。\n",
        "```\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"query\":RunnablePassthrough(),\n",
        "        \"context\":retriever\n",
        "```\n",
        "この部分は、以下の書き方の省略形になっている。\n",
        "```\n",
        "qa_chain = (\n",
        "    RunnableParallel({\n",
        "      \"query\":RunnablePassthrough(),\n",
        "      \"context\": retriever})\n",
        "    )\n",
        "```\n",
        "`RunnableParallel` は複数の Runnable に同じ入力を渡して並行して実行し、結果をそれぞれのキーの値として格納した辞書を出力する。  \n",
        "[langchain_core.runnables.base.RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)  \n",
        "\n",
        "```\n",
        "result = qa_chain.invoke(query)\n",
        "```\n",
        "ここで `qa_chain` に入力された `query` は、`RunnableParallel` に渡され、`retriever` によって `query` と類似したドキュメントが検索されて、プロンプトの変数 `context` に代入される値として `chat_prompt` に渡される。並行して、`query` の値はプロンプトの変数 `query` に代入されるべき値として `chat_prompt` に渡される。  \n",
        "\n",
        "`RunnablePassthrough` は受け取った入力をそのまま出力する。  \n",
        "`qa_chain` の 2 番目にある `chat_prompt` はユーザーの入力を直接受け取ることができないため、`RunnablePassthrough` によってユーザーの入力を受け渡される必要がある。  \n",
        "[langchain_core.runnables.passthrough.RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "h8n15XSKWGpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RunnableParallel の動作\n",
        "*  `RunnableParallel` も Runnable であり、`invoke` メソッドで実行できる。\n",
        "*  ここでは `1` という入力に対して 2 つの処理を実行する例で動作を確認してみる。"
      ],
      "metadata": {
        "id": "uBolYJnDfpHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    origin=RunnablePassthrough(),\n",
        "    modified=lambda x: x+1\n",
        ")\n",
        "\n",
        "runnable.invoke(1)"
      ],
      "metadata": {
        "id": "dfPbN3rwfd5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: RAG の処理を関数化して実行する\n",
        "---"
      ],
      "metadata": {
        "id": "8vl4ko_WSLfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 関数を定義"
      ],
      "metadata": {
        "id": "uVidnZU2JSS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ドキュメントのロード"
      ],
      "metadata": {
        "id": "sGrOFjVYe8gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain_community.document_loaders.text import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data"
      ],
      "metadata": {
        "id": "jSvyGZMvNOMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wikipedia\n",
        "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "    data = loader.load()\n",
        "    return data"
      ],
      "metadata": {
        "id": "xCHH2XydeYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter での処理を関数にラップする"
      ],
      "metadata": {
        "id": "nsdhIlhCe0YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_data(data, chunk_size=256, chunk_overlap=0):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ewvGumJ0enHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding のコスト計算"
      ],
      "metadata": {
        "id": "j7TJHrxCfTvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    # check prices here: https://openai.com/pricing\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
      ],
      "metadata": {
        "id": "uiiEmUA4fSuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector store\n",
        "新規の Index を作成、あるいは既存の Index をロードして、Vector store オブジェクトを作成する。"
      ],
      "metadata": {
        "id": "srBEYDTBflJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # Importing the necessary libraries and initializing the Pinecone client\n",
        "    import pinecone\n",
        "    from langchain_pinecone import PineconeVectorStore\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from pinecone import PodSpec, ServerlessSpec\n",
        "\n",
        "\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Loading from existing index\n",
        "    if index_name in pc.list_indexes().names():\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ')\n",
        "        vector_store = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # Creating the index and embedding the chunks into the index\n",
        "        print(f'Creating index {index_name} and embeddings ... ')\n",
        "\n",
        "        # Creating a new index\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region='us-east-1'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "        # Inserting the embeddings into the index and returning a new Pinecone vector store instance.\n",
        "        vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "szv_vHrofgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index の削除 (上と同じ)"
      ],
      "metadata": {
        "id": "HcYOmeAbgh8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')"
      ],
      "metadata": {
        "id": "bbZzJ4fyggzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain"
      ],
      "metadata": {
        "id": "sD9Xzc8_1mQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(vector_store, q, k=5):\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from operator import itemgetter\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "    # Chat model\n",
        "    model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    # Retriever\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    # Prompt template\n",
        "    system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "    {context}\"\"\"\n",
        "    human_message = \"質問：{query}\"\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "        (\n",
        "            \"system\", system_message\n",
        "        ),\n",
        "        (\n",
        "            \"human\", human_message\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Chain の定義\n",
        "    chain = (\n",
        "        {\n",
        "            \"query\":itemgetter('query')|RunnablePassthrough(),\n",
        "            \"context\":itemgetter('query')|retriever\n",
        "        }\n",
        "        |chat_prompt\n",
        "        |model\n",
        "        |StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Chain の実行\n",
        "    answer = chain.invoke({'query':q})\n",
        "    return answer"
      ],
      "metadata": {
        "id": "1F6lpsPfg3VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 処理の実行  \n",
        "上で定義した関数を実行する"
      ],
      "metadata": {
        "id": "OcURXr0v1yCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_document/load_wikipedia を実行"
      ],
      "metadata": {
        "id": "DhkvLJNJJ8w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_document('files/employment_regulations.pdf')\n",
        "\n",
        "# print(data[1].page_content)\n",
        "# print(data[10].metadata)\n",
        "\n",
        "print(f'You have {len(data)} pages in your data')\n",
        "print(f'There are {len(data[10].page_content)} characters in the page')"
      ],
      "metadata": {
        "id": "6keUaWOlw2pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### chunk_data を実行"
      ],
      "metadata": {
        "id": "lCDkaKajKElT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_data(data, chunk_size=300, chunk_overlap=0)\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "lEA1rxeRxEEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_embedding_cost(chunks)"
      ],
      "metadata": {
        "id": "uuw4EMvDxFG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### delete_pinecone_index を実行"
      ],
      "metadata": {
        "id": "XXpjVpOgKLXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete_pinecone_index()"
      ],
      "metadata": {
        "id": "8cjQkI95xKM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### insert_or_fetch_embeddings を実行"
      ],
      "metadata": {
        "id": "hzdJPfTXKRoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'askadocument'\n",
        "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
      ],
      "metadata": {
        "id": "Vmkn0nBtxNj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_answer をインタラクティブな形で実行"
      ],
      "metadata": {
        "id": "pEoYp80zKYjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = get_answer(vector_store, q)\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')"
      ],
      "metadata": {
        "id": "pLm4DgGXxlgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: RAG Chain に会話履歴を追加する"
      ],
      "metadata": {
        "id": "oCZuMnDDi_u_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain に Chat history を組み込む  \n",
        "RunnableWithMessageHistory クラスを使用して Chain の入力に Chat history を組み込む"
      ],
      "metadata": {
        "id": "feSkDKebLVhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_with_history1(vector_store, q, chat_history, session_id='unused', k=5):\n",
        "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from operator import itemgetter\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "    from langchain_core.chat_history import BaseChatMessageHistory\n",
        "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "    # Chat model\n",
        "    model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    # Retriever\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    # Prompt template\n",
        "    system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "    {context}\"\"\"\n",
        "    human_message = \"質問：{query}\"\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "        (\n",
        "            \"system\", system_message\n",
        "        ),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\n",
        "            \"human\", human_message\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # ユーザーのクエリと retriever が取得した Documents を出力する Runnable\n",
        "    add_context = RunnablePassthrough.assign(context=itemgetter(\"query\") | retriever)\n",
        "\n",
        "    # add_context のテスト出力\n",
        "    test_output = add_context.invoke({'query': q})\n",
        "    print(f\"TestOutput: {test_output}\")\n",
        "    # テスト出力には含まれないが、以下の処理では RunnableWithMessageHistory によって \"chat_history\" キーにチャット履歴が渡される\n",
        "\n",
        "    # Chain を定義\n",
        "    rag_chain = add_context | chat_prompt | model | StrOutputParser()\n",
        "\n",
        "    # RunnableWithMessageHistory: Chat history を入力に追加して Chain を実行するインスタンス\n",
        "    runnable_with_history = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        lambda session_id: chat_history, # session_id を受け取って対応する chat message history インスタンス (BaseChatMessageHistory) を返す関数\n",
        "        input_messages_key=\"query\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "    )\n",
        "\n",
        "    # Chain の実行\n",
        "    answer = runnable_with_history.invoke({'query': q}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "    #answer = runnable_with_history.invoke({'query': q}, config={\"configurable\": {\"session_id\": session_id}, 'callbacks': [ConsoleCallbackHandler()]})\n",
        "    return answer"
      ],
      "metadata": {
        "id": "XyRxnRAK2y4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`runnable_with_history` の実行では、以下のような処理が行われる  \n",
        "\n",
        "1. `RunnableWithMessageHistory` により `{'query': <ユーザーのクエリ>, 'chat_history': <会話履歴の Messages>}` が `add_context` に入力される\n",
        "2. `add_context` が `<ユーザーのクエリ>` を `retriever` に渡して関連する Documents を取得し、`'query': <ユーザーのクエリ>, 'chat_history': <会話履歴の Messages>, context: <retriever が取得した Documents>}` を出力する\n",
        "3. `chat_prompt` がそれを入力として受け取り、テンプレートの各変数 `context` `query` `chat_history`に対応するキーの値を代入して、PromptValue を出力する\n",
        "4. `model` がそれを入力として受け取り、LLM の API を呼び出す\n",
        "\n"
      ],
      "metadata": {
        "id": "v9KFXH0oRNey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "# Chat history\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = get_answer_with_history1(vector_store, q, chat_history, 'b234')\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')"
      ],
      "metadata": {
        "id": "_nuPA_xk5n_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever にも会話履歴を考慮させる\n",
        "*  上の方法では、モデルに最終的に渡すプロンプトに会話履歴を含めることはできるが、Retriever にドキュメントを検索させる際には会話履歴を含まない最新の入力だけが使われるため、必ずしも適切なドキュメントが検索されないことがある (前の会話を受けた指示語が入力文に含まれている場合など)\n",
        "*  履歴を含んだ入力から、いったんモデルを使って検索のための文を生成し、それを使って Retriever に検索させることで適切なドキュメントを検索させることができる。"
      ],
      "metadata": {
        "id": "iZGTpqxmiXWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_with_history2(vector_store, q, chat_history, session_id='unused', k=20):\n",
        "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from operator import itemgetter\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "    from langchain_core.chat_history import BaseChatMessageHistory\n",
        "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "    from langchain.chains import create_history_aware_retriever\n",
        "\n",
        "    # Chat model\n",
        "    model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    # Retriever\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    # 検索用にクエリを書き換えるためのプロンプト\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood \"\n",
        "        \"without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 会話履歴を考慮する Retriever を作成\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        model, retriever, contextualize_q_prompt\n",
        "    )\n",
        "\n",
        "    # ユーザーのクエリに回答させるための Prompt template\n",
        "    system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "    {context}\"\"\"\n",
        "    human_message = \"質問：{input}\"\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "        (\n",
        "            \"system\", system_message\n",
        "        ),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\n",
        "            \"human\", human_message\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # ユーザーのクエリと history_aware_retriever が取得した Documents を出力する Runnable\n",
        "    add_context = RunnablePassthrough.assign(context=history_aware_retriever)\n",
        "\n",
        "    # Chain を定義\n",
        "    rag_chain = add_context | chat_prompt | model | StrOutputParser()\n",
        "\n",
        "    runnable_with_history = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        lambda session_id: chat_history, # session_id を受け取って対応する chat message history インスタンス (BaseChatMessageHistory) を返す関数\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "    )\n",
        "\n",
        "    # Chain の実行\n",
        "    answer = runnable_with_history.invoke({'input': q}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "    # answer = runnable_with_history.invoke({'input': q}, config={\"configurable\": {\"session_id\": session_id}, 'callbacks': [ConsoleCallbackHandler()]})\n",
        "    return answer"
      ],
      "metadata": {
        "id": "tynaoizHMu2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "# Chat history\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = get_answer_with_history2(vector_store, q, chat_history, 'a234')\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')"
      ],
      "metadata": {
        "id": "y2VY7YpcOlKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 参考：組み込み関数による Chain の作成\n",
        "次のように、LangChain に組み込みの関数を使って Chain を作成することもできる。  \n",
        "ただし、ユーザーからの入力や Chain からの出力のキーが既定のものに制約される。  \n",
        "[langchain.chains.retrieval.create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html)  \n",
        "[langchain.chains.combine_documents.stuff.create_stuff_documents_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)"
      ],
      "metadata": {
        "id": "t2P3AjdT83OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_with_history3(vector_store, q, chat_history, session_id='unused', k=10):\n",
        "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from operator import itemgetter\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "    from langchain_core.chat_history import BaseChatMessageHistory\n",
        "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "    from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "    from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "\n",
        "    # Chat model\n",
        "    model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "    # Retriever\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    # Chat template\n",
        "    system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "    {context}\"\"\"\n",
        "    human_message = \"質問：{input}\" # retriever (BaseRetriever のサブクラス) を使用する場合、ユーザーからの入力のキーは \"input\" にする必要がある\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "        (\n",
        "            \"system\", system_message\n",
        "        ),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\n",
        "            \"human\", human_message\n",
        "        )\n",
        "    ])\n",
        "    # Q and A 部分の Chain を作る\n",
        "    question_answer_chain = create_stuff_documents_chain(model, chat_prompt)\n",
        "\n",
        "    # Retriever を組み込んだ全体の Chain を作る\n",
        "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "    runnable_with_history = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        lambda session_id: chat_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\", # 組み込み関数で作成した rag_chain の出力では \"answer\" キーの値として回答本文が格納される\n",
        "    )\n",
        "\n",
        "    # Chain の実行\n",
        "    answer = runnable_with_history.invoke({'input': q}, config={\"configurable\": {\"session_id\": session_id}})[\"answer\"]\n",
        "    #answer = runnable_with_history.invoke({'query': q}, config={\"configurable\": {\"session_id\": session_id}, 'callbacks': [ConsoleCallbackHandler()]})\n",
        "    return answer"
      ],
      "metadata": {
        "id": "DvPR-pZsvknw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "# Chat history\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer = get_answer_with_history3(vector_store, q, chat_history, 'a345')\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')"
      ],
      "metadata": {
        "id": "3QzS0vFgxCie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}